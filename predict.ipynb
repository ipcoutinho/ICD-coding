{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "94kJ5jKXEY-p"
      },
      "source": [
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "    import sklearn\n",
        "    import h5py\n",
        "    import tensorflow.keras\n",
        "\n",
        "import numpy as np    \n",
        "np.random.seed(1337) # for reproducibility\n",
        "\n",
        "import tensorflow as tf \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt') \n",
        "import codecs\n",
        "import jellyfish\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from nltk import tokenize\n",
        "from attention import AttLayer\n",
        "\n",
        "# Set parameters:\n",
        "max_features = 150000           # Maximum number of tokens in vocabulary\n",
        "maxlen = 27                     # Maximum length of each sentence \n",
        "maxsents = 211                  # Maximum number of sentences    \n",
        "\n",
        "# Defining the loss function as the conjugation of the binary cross-entropy (BCE) with the log-cosh Tvsersky loss (LCTL):\n",
        "def LCTL(y_true, y_pred, beta):\n",
        "    y_true = tf.compat.v1.layers.flatten(y_true)\n",
        "    y_true = tf.math.round(y_true)\n",
        "    y_pred = tf.compat.v1.layers.flatten(y_pred)\n",
        "    numerator = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "    denominator = y_true * y_pred + beta * (1 - y_true) * y_pred + (1 - beta) * y_true * (1 - y_pred)\n",
        "    loss =  1 - (numerator + 1) / (tf.reduce_sum(denominator, axis=-1) + 1)\n",
        "    loss = K.log((K.exp(loss) + K.exp(-loss)) / 2.0)\n",
        "    return loss\n",
        "\n",
        "BCE = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def loss_function(alpha, beta):\n",
        "    def loss(y_true, y_pred):\n",
        "        return alpha * BCE(y_true, y_pred) + (1 - alpha) * LCTL(y_true, y_pred, beta)\n",
        "    return loss\n",
        "\n",
        "# Load dictionary\n",
        "word_index = np.load('DICT.npy', allow_pickle=True).item()\n",
        "\n",
        "# Load ICD to integer codes dictionary\n",
        "le = np.load('LABEL_ENCODER.npy', allow_pickle=True).item()\n",
        "\n",
        "print('Load model...')\n",
        "model = load_model('model.h5', custom_objects = {\"AttLayer\": AttLayer, \"loss\": loss_function(alpha=0.9, beta=0.5)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_rfIFmHeDv"
      },
      "source": [
        "print('Loading data...')\n",
        "\n",
        "texts_test = [ line.rstrip('\\n') for line in codecs.open('example_test.txt', encoding=\"utf-8\") ]    \n",
        "\n",
        "# Discharge summaries\n",
        "ds = []\n",
        "for i in range (maxsents):\n",
        "    ds.append([ line.split('<>')[i] for line in texts_test ])\n",
        "\n",
        "print('Computing Testing Set...')\n",
        "\n",
        "X_test = np.zeros((len(ds[0]), maxsents, maxlen), dtype = 'int32')\n",
        "\n",
        "print('Loading discharge summaries...')\n",
        "\n",
        "for m in range(maxsents):\n",
        "    part = ds[m]\n",
        "    for i, sentences in enumerate(part):\n",
        "        sentences = tokenize.sent_tokenize( sentences )\n",
        "        k = 0\n",
        "        for j, sent in enumerate(sentences):\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            for _ , word in enumerate(wordTokens):\n",
        "                if word_index.get(word) == None: \n",
        "                    aux = [(jellyfish.jaro_winkler(k,word),v) for k,v in word_index.items()]\n",
        "                    if k < maxlen and max(aux)[1] < max_features:\n",
        "                        X_test[i,m,k] = max(aux)[1]\n",
        "                        k = k + 1\n",
        "                else:\n",
        "                    if k < maxlen and word_index.get(word) < max_features:\n",
        "                        X_test[i,m,k] = word_index.get(word)\n",
        "                        k = k + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5SqMq1uViVm"
      },
      "source": [
        "print('X_test shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtRbsjGWVlGj"
      },
      "source": [
        "np.save('X_test.npy', X_test) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgbtv2uoElHQ"
      },
      "source": [
        "print('Predicting...')\n",
        "\n",
        "prediction_matrix = model.predict(X_test, batch_size=3)\n",
        "\n",
        "y_pred = np.round(prediction_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9grni80WBO4"
      },
      "source": [
        "np.save('prediction_matrix.npy', prediction_matrix)\r\n",
        "np.save('y_pred.npy', y_pred)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wApmRXjeRrk"
      },
      "source": [
        "print('Writing output...')\n",
        "\n",
        "pred = []\n",
        "\n",
        "for i in range (len(prediction_matrix)):\n",
        "    pred_i = []\n",
        "    for j in range (len(prediction_matrix[0])):\n",
        "        if prediction_matrix[i][j]>0.5:\n",
        "            pred_i.append(le.inverse_transform([j])[0])\n",
        "    pred.append(pred_i)\n",
        "\n",
        "np.savetxt('predictions.npy', pred, delimiter=\" \", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}