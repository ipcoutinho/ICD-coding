{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eukDRmlvs49a"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337) # for reproducibility\n",
        "\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "    import sklearn\n",
        "    import h5py     \n",
        "    import tensorflow.keras\n",
        "\n",
        "import tensorflow as tf\n",
        "    \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pickle\n",
        "import os\n",
        "import codecs\n",
        "import theano\n",
        "import jellyfish\n",
        "import gc\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import collections as col\n",
        "from collections import Counter \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Masking\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.python.keras.layers import Input\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from sklearn import model_selection\n",
        "from nltk import tokenize\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import Counter\n",
        "from attention import AttLayer\n",
        "\n",
        "earlyStopping = EarlyStopping(patience=2, verbose=1, monitor='val_loss', restore_best_weights=True)         \n",
        "\n",
        "# Set parameters:\n",
        "max_features = 150000           # Maximum number of tokens in vocabulary\n",
        "maxlen = 27                     # Maximum length of each sentence \n",
        "maxsents = 211                  # Maximum number of sentences\n",
        "batch_size = 32                 # Batch size given to the model while training\n",
        "embedding_dims = 175            # Embedding dimensions\n",
        "nb_epoch = 50                   # Number of epochs for training\n",
        "gru_output_size = 175           # GRU output dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVulKrUM-JSo"
      },
      "source": [
        "# Get unique values from a list\n",
        "def unique(list1): \n",
        "  \n",
        "    unique_list = [] \n",
        "      \n",
        "    for x in list1: \n",
        "        if x not in unique_list: \n",
        "            unique_list.append(x) \n",
        "            \n",
        "    return unique_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-DkJRZpfXow"
      },
      "source": [
        "print('Loading data...')\n",
        "\n",
        "texts_train = [ line.rstrip('\\n') for line in codecs.open('example_train.txt', encoding=\"utf-8\") ]\n",
        "texts_val = [ line.rstrip('\\n') for line in codecs.open('example_val.txt', encoding=\"utf-8\") ]    \n",
        "\n",
        "texts = texts_train + texts_val\n",
        "\n",
        "# List of ICD full-codes for each discharge summary:\n",
        "labels = [ line.split('<>')[maxsents].replace(\"'\",\"\") for line in texts ]\n",
        "labels = [x[2:-2] for x in labels]\n",
        "labels = [x.split(', ') for x in labels]\n",
        "\n",
        "# Remove repeated labels\n",
        "for i in range (len(labels)):\n",
        "    labels[i] = unique(labels[i])\n",
        "\n",
        "# Using sklearn package attribute an integer to each code that occures resulting in:\n",
        "le = preprocessing.LabelEncoder() \n",
        "\n",
        "char = le.fit([item for sublist in labels for item in sublist]) \n",
        "\n",
        "labels_int = np.copy(labels)\n",
        "\n",
        "for i in range(len(labels_int)):\n",
        "    labels_int[i] = le.transform(labels_int[i])\n",
        "\n",
        "# Conversion of the ICD code into a one-hot vector\n",
        "# e.g. diagnosis code D250.00 (in labels) -> 2 (in labels_int) -> [0, 0, 1, 0, ..., 0] (in labels_1hot)\n",
        "\n",
        "num_classes=1+max([max(x) for x in labels_int])  \n",
        "\n",
        "labels_1hot = np.zeros((len(labels_int), num_classes), dtype=np.float64)\n",
        "\n",
        "for i in range(labels_1hot.shape[0]):\n",
        "    labels_1hot[i,:] = sum(to_categorical(labels_int[i],num_classes))\n",
        "\n",
        "y_train = labels_1hot[:len(texts_train)]\n",
        "y_val = labels_1hot[len(texts_train):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMr0xfbi1sYf"
      },
      "source": [
        "np.save('LABEL_ENCODER.npy', le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81K-PU4TjflP"
      },
      "source": [
        "print('Multi-label smothing regularization...')\n",
        "\n",
        "# This strategy consists of attributing the value y=0.05 to codes belonging to the same blocks as the identified codes\n",
        "# A block corresponds to the 3-digit version (diagnosis code) or 2-digit version (procedure code)\n",
        "\n",
        "# List of ICD blocks for each discharge summary:\n",
        "labels_block = []\n",
        "for i in range (len(texts_train)):\n",
        "    labels_block.append(labels[i].copy())\n",
        "\n",
        "for i in range(len(labels_block)):\n",
        "    for j in range(len(labels_block[i])):\n",
        "        if labels_block[i][j][0] =='D' and labels_block[i][j][1] =='E':\n",
        "            labels_block[i][j] = labels_block[i][j][:5]\n",
        "        elif labels_block[i][j][0] =='D'and labels_block[i][j][1] !='E':\n",
        "            labels_block[i][j] = labels_block[i][j][:4]\n",
        "        else:\n",
        "            labels_block[i][j] = labels_block[i][j][:3]\n",
        "\n",
        "# List of the corresponding block of each identified ICD code:\n",
        "classes_block = []\n",
        "for i in range (num_classes):\n",
        "    code = le.inverse_transform([i])[0]\n",
        "    if code[0] =='D' and code[1] =='E':\n",
        "        code_block = code[:5]\n",
        "    elif code[0] =='D'and code[1] !='E':\n",
        "        code_block = code[:4]\n",
        "    else:\n",
        "        code_block = code[:3]\n",
        "    classes_block.append(code_block)\n",
        "\n",
        "for i in range (y_train.shape[0]):\n",
        "    blocks = labels_block[i]\n",
        "    for j in range (y_train.shape[1]):\n",
        "        if y_train[i][j]==0:\n",
        "            if classes_block[j] in blocks:\n",
        "                y_train[i][j]=0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpSzefRE1wyG"
      },
      "source": [
        "print('y_train shape:', y_train.shape)\n",
        "print('y_val shape:', y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD1IxustSkSY"
      },
      "source": [
        "# Discharge summaries\n",
        "ds = []\n",
        "for i in range (maxsents):\n",
        "    ds.append([ line.split('<>')[i] for line in texts ])\n",
        "\n",
        "# Spliting the discharge summaries into a training set and a validation set_\n",
        "\n",
        "# Training set\n",
        "X_train_ds = []\n",
        "for i in range (maxsents):\n",
        "    X_train_ds_i = []\n",
        "    for j in range (len(texts_train)):\n",
        "        X_train_ds_i.append(ds[i][j])\n",
        "    X_train_ds.append(X_train_ds_i)\n",
        "\n",
        "# Validation set\n",
        "X_val_ds = []\n",
        "for i in range (maxsents):\n",
        "    X_val_ds_i = []\n",
        "    for j in range (len(texts_train),len(texts)):\n",
        "        X_val_ds_i.append(ds[i][j])\n",
        "    X_val_ds.append(X_val_ds_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hiarRBFS5tC"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = max_features)\n",
        "tokenizer.fit_on_texts([item for sublist in X_train_ds for item in sublist])\n",
        "\n",
        "# Attribute an integer to each token that occures in the texts \n",
        "# Conversion of each dataset entry in a (maxsents, maxlen) shape matrix resulting in variables:\n",
        "\n",
        "print('Computing training set...')\n",
        "\n",
        "X_train = np.zeros((len(X_train_ds[0]), maxsents, maxlen), dtype = 'int32')\n",
        "\n",
        "print('Loading discharge summaries ...')\n",
        "\n",
        "for m in range(maxsents):\n",
        "    part = X_train_ds[m]\n",
        "    for i, sentences in enumerate(part):\n",
        "        sentences = tokenize.sent_tokenize( sentences )\n",
        "        k = 0\n",
        "        for j, sent in enumerate(sentences):\n",
        "            if j < maxsents:\n",
        "                wordTokens = text_to_word_sequence(sent)\n",
        "                for _ , word in enumerate(wordTokens):\n",
        "                    if k < maxlen and tokenizer.word_index[word] < max_features:\n",
        "                        X_train[i,m,k] = tokenizer.word_index[word]\n",
        "                        k = k + 1\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "np.save('DICT.npy', word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HNTgN7BTAQh"
      },
      "source": [
        "print('Computing validation set...')\n",
        "\n",
        "X_val = np.zeros((len(X_val_ds[0]), maxsents, maxlen), dtype = 'int32')\n",
        "\n",
        "print('Loading discharge summaries...')\n",
        "\n",
        "for m in range(maxsents):\n",
        "    part = X_val_ds[m]\n",
        "    for i, sentences in enumerate(part):\n",
        "        sentences = tokenize.sent_tokenize( sentences )\n",
        "        k = 0\n",
        "        for j, sent in enumerate(sentences):\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            for _ , word in enumerate(wordTokens):\n",
        "                if word_index.get(word) == None: \n",
        "                    aux = [(jellyfish.jaro_winkler(k,word),v) for k,v in word_index.items()]\n",
        "                    if k < maxlen and max(aux)[1] < max_features:\n",
        "                        X_val[i,m,k] = max(aux)[1]\n",
        "                        k = k + 1\n",
        "                else:\n",
        "                    if k < maxlen and word_index.get(word) < max_features:\n",
        "                        X_val[i,m,k] = word_index.get(word)\n",
        "                        k = k + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpEpAveCSqjk"
      },
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('X_val shape:', X_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHtE_4eDpSO"
      },
      "source": [
        "print('Loading pretrained word embeddings ...')\r\n",
        "\r\n",
        "#embedding_matrix = np.load('embeddins_matrix.npy')\r\n",
        "\r\n",
        "# For demonstratrion purposes let us assume a embedding matrix as an array filled with zeros:\r\n",
        "embedding_matrix = np.zeros((len(word_index)+1, embedding_dims))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW9RBZq2og0G"
      },
      "source": [
        "# Defining the loss function as the conjugation of the binary cross-entropy (BCE) with the log-cosh Tvsersky loss (LCTL):\n",
        "def LCTL(y_true, y_pred, beta):\n",
        "    y_true = tf.compat.v1.layers.flatten(y_true)\n",
        "    y_true = tf.math.round(y_true)\n",
        "    y_pred = tf.compat.v1.layers.flatten(y_pred)\n",
        "    numerator = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "    denominator = y_true * y_pred + beta * (1 - y_true) * y_pred + (1 - beta) * y_true * (1 - y_pred)\n",
        "    loss =  1 - (numerator + 1) / (tf.reduce_sum(denominator, axis=-1) + 1)\n",
        "    loss = K.log((K.exp(loss) + K.exp(-loss)) / 2.0)\n",
        "    return loss\n",
        "\n",
        "BCE = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def loss_function(alpha, beta):\n",
        "    def loss(y_true, y_pred):\n",
        "        return alpha * BCE(y_true, y_pred) + (1 - alpha) * LCTL(y_true, y_pred, beta)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBoQSIJjt7Pq"
      },
      "source": [
        "print('Build model...')\n",
        "\n",
        "# Inputs\n",
        "review_input = Input(shape=(maxsents,maxlen), dtype='int32')\n",
        "\n",
        "# Embedding Layer\n",
        "embedding_layer = Embedding(len(word_index)+1, embedding_dims, \n",
        "                            input_length=maxlen,\n",
        "                            weights=[embedding_matrix])\n",
        "\n",
        "# WORD-LEVEL\n",
        "sentence_input = Input(shape=(maxlen,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "\n",
        "# Bidirectional GRU\n",
        "l_gru = Bidirectional(GRU(gru_output_size, return_sequences=True))(embedded_sequences)\n",
        "l_dense = TimeDistributed(Dense(units=gru_output_size))(l_gru)\n",
        "\n",
        "# Word-Level Attention Layer\n",
        "l_att = AttLayer()(l_dense)\n",
        "sentEncoder = Model(sentence_input, l_att)\n",
        "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
        "\n",
        "# SENTENCE_LEVEL\n",
        "# Bidirectional GRU\n",
        "l_gru_sent = Bidirectional(GRU(gru_output_size, return_sequences=True))(review_encoder)\n",
        "l_dense_sent = TimeDistributed(Dense(units=gru_output_size))(l_gru_sent)\n",
        "\n",
        "# Sentence-Level Attention Layer\n",
        "postp = AttLayer()(l_dense_sent)\n",
        "\n",
        "# Embedding Average\n",
        "sentEmbed = Model(sentence_input, embedded_sequences)\n",
        "review_fasttext = TimeDistributed(sentEmbed)(review_input)\n",
        "fasttext = GlobalAveragePooling2D()(review_fasttext)\n",
        "\n",
        "postp_aux = tensorflow.keras.layers.Concatenate( axis = 1 )( [ postp , fasttext ] )\n",
        "\n",
        "postp_aux_drop = Dropout(0.05)(postp_aux)\n",
        "\n",
        "postp = Dense(units=(gru_output_size+embedding_dims))(postp_aux_drop)\n",
        "\n",
        "# Sigmoid Layer\n",
        "preds_aux = Dense(units=num_classes, activation='sigmoid')(postp)\n",
        "\n",
        "model = Model(inputs = review_input, outputs = preds_aux)\n",
        "\n",
        "model.compile(loss=[loss_function(alpha=0.9, beta=0.5)], optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_val, y_val), \n",
        "          epochs=nb_epoch,\n",
        "          batch_size=batch_size,\n",
        "          callbacks=[earlyStopping])\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}